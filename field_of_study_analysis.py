# -*- coding: utf-8 -*-
"""Field of Study Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTbVyFD_GMqmiRGVMOtx2ikklRq1kozJ
"""

# MIS 581
# Capstone - Whether Quality of Institution or Field of Study Better Predicts Student Loan Repayment
# Graham Burns
# Field of Study Analysis

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix
from imblearn.over_sampling import SMOTE

# Load dataset
data_path = '/content/Most-Recent-Cohorts-Field-of-Study.csv'
data = pd.read_csv(data_path, low_memory=False)

# Select relevant columns
columns = ['BBRR3_FED_COMP_PAIDINFULL', 'CIPCODE']
data = data[columns].dropna()

# Rename them for clarity
data.rename(columns={'BBRR3_FED_COMP_PAIDINFULL': 'loan_repaid_fully',
                     'CIPCODE': 'area_of_study'}, inplace=True)

# Convert CIPCODE to broad areas of study (2-digit level)
data['area_of_study'] = data['area_of_study'].astype(str).str[:2]

# Convert categorical variables to numeric for Area of Study
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_areas = encoder.fit_transform(data[['area_of_study']])
area_of_study_columns = encoder.get_feature_names_out(['area_of_study'])
data_encoded_areas = pd.DataFrame(encoded_areas, columns=area_of_study_columns)

# Remove original categorical column and merge encoded columns
data = data.drop(columns=['area_of_study'])
data = pd.concat([data, data_encoded_areas], axis=1)

# Convert loan repayment column to numeric
data['loan_repaid_fully'] = pd.to_numeric(data['loan_repaid_fully'], errors='coerce')

# Create binary target variable
data['loan_repaid'] = (data['loan_repaid_fully'] > 0).astype(int)

# Drop the original repayment column
X = data.drop(columns=['loan_repaid_fully', 'loan_repaid'])
y = data['loan_repaid']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Address class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Train Logistic Regression Model
logreg = LogisticRegression(max_iter=500)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)

# Train Random Forest Model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Train XGBoost Model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

# Model evaluation
def evaluate_model(y_true, y_pred, model_name):
    print(f"{model_name} Accuracy: {accuracy_score(y_true, y_pred):.3f}")
    print(f"{model_name} Precision: {precision_score(y_true, y_pred, zero_division=1):.3f}")
    print(f"{model_name} Recall: {recall_score(y_true, y_pred, zero_division=1):.3f}")
    print(f"{model_name} ROC AUC: {roc_auc_score(y_true, y_pred):.3f}\n")

evaluate_model(y_test, y_pred_logreg, "Logistic Regression")
evaluate_model(y_test, y_pred_rf, "Random Forest")
evaluate_model(y_test, y_pred_xgb, "XGBoost")

# Feature importance
feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Display top features
print("Top Features Influencing Loan Repayment:")
print(feature_importances.head(10))

# MIS 581
# Capstone - Whether Quality of Institution or Field of Study Better Predicts Student Loan Repayment
# Graham Burns
# Quality of Institution Analysis against other Factors

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix

# Load dataset
data_path = '/content/Most-Recent-Cohorts-Field-of-Study.csv'
data = pd.read_csv(data_path, low_memory=False)

# Select relevant columns
columns = ['BBRR3_FED_COMP_PAIDINFULL', 'BBRR3_FED_COMP_MAKEPROG', 'CIPCODE',
           'EARN_MDN_HI_1YR', 'EARN_MDN_HI_2YR', 'DEBT_ALL_PP_ANY_MDN', 'DEBT_ALL_PP_ANY_MDN10YRPAY']
data = data[columns].dropna()

# Rename them for clarity
data.rename(columns={'BBRR3_FED_COMP_PAIDINFULL': 'loan_repaid_fully',
                     'BBRR3_FED_COMP_MAKEPROG': 'loan_repaid_progress',
                     'CIPCODE': 'area_of_study',
                     'EARN_MDN_HI_1YR': 'median_earnings_1yr',
                     'EARN_MDN_HI_2YR': 'median_earnings_2yr',
                     'DEBT_ALL_PP_ANY_MDN': 'graduate_median_debt',
                     'DEBT_ALL_PP_ANY_MDN10YRPAY': 'monthly_loan_payment'}, inplace=True)

# Convert CIPCODE to broad areas of study (2-digit level)
data['area_of_study'] = data['area_of_study'].astype(str).str[:2]

# Convert categorical variables to numeric for Area of Study
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_areas = encoder.fit_transform(data[['area_of_study']])
area_of_study_columns = encoder.get_feature_names_out(['area_of_study'])
data_encoded_areas = pd.DataFrame(encoded_areas, columns=area_of_study_columns)

# Remove original categorical column and merge encoded columns
data = data.drop(columns=['area_of_study'])
data = pd.concat([data, data_encoded_areas], axis=1)

# Convert loan repayment columns to numeric
data['loan_repaid_fully'] = pd.to_numeric(data['loan_repaid_fully'], errors='coerce')
data['loan_repaid_progress'] = pd.to_numeric(data['loan_repaid_progress'], errors='coerce')

# Create binary target variable
data['loan_repaid'] = ((data['loan_repaid_fully'] > 0) | (data['loan_repaid_progress'] > 0)).astype(int)

# Define numeric features
numeric_features = ['median_earnings_1yr', 'median_earnings_2yr', 'graduate_median_debt', 'monthly_loan_payment']
for feature in numeric_features:
    data[feature] = data[feature].replace(['PrivacySuppressed', 'PS'], np.nan)
    data[feature] = pd.to_numeric(data[feature], errors='coerce')

# Drop rows with NaN values
data = data.dropna(subset=numeric_features)

# Scale numeric features
scaler = MinMaxScaler()
data[numeric_features] = scaler.fit_transform(data[numeric_features])

# Split dataset
X = data.drop(columns=['loan_repaid_fully', 'loan_repaid_progress', 'loan_repaid'])
y = data['loan_repaid']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression Model
logreg = LogisticRegression(max_iter=500)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)

# Train Random Forest Model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Model evaluation
def evaluate_model(y_true, y_pred, model_name):
    print(f"{model_name} Accuracy: {accuracy_score(y_true, y_pred):.3f}")
    print(f"{model_name} Precision: {precision_score(y_true, y_pred):.3f}")
    print(f"{model_name} Recall: {recall_score(y_true, y_pred):.3f}")
    print(f"{model_name} ROC AUC: {roc_auc_score(y_true, y_pred):.3f}\n")

evaluate_model(y_test, y_pred_logreg, "Logistic Regression")
evaluate_model(y_test, y_pred_rf, "Random Forest")

# Feature importance
feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Display top features
print("Top Features Influencing Loan Repayment:")
print(feature_importances.head(10))